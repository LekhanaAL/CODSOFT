{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8c18fd-ce50-4bfe-8b2b-4ca809ff23db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "403a0e47-f6cd-4449-9788-d03535396237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('fraudTest.csv')\n",
    "data = pd.read_csv('fraudTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2ac118-d259-4e80-bd2c-e5b947189137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1296675 entries, 0 to 1296674\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   Unnamed: 0             1296675 non-null  int64  \n",
      " 1   trans_date_trans_time  1296675 non-null  object \n",
      " 2   cc_num                 1296675 non-null  int64  \n",
      " 3   merchant               1296675 non-null  object \n",
      " 4   category               1296675 non-null  object \n",
      " 5   amt                    1296675 non-null  float64\n",
      " 6   first                  1296675 non-null  object \n",
      " 7   last                   1296675 non-null  object \n",
      " 8   gender                 1296675 non-null  object \n",
      " 9   street                 1296675 non-null  object \n",
      " 10  city                   1296675 non-null  object \n",
      " 11  state                  1296675 non-null  object \n",
      " 12  zip                    1296675 non-null  int64  \n",
      " 13  lat                    1296675 non-null  float64\n",
      " 14  long                   1296675 non-null  float64\n",
      " 15  city_pop               1296675 non-null  int64  \n",
      " 16  job                    1296675 non-null  object \n",
      " 17  dob                    1296675 non-null  object \n",
      " 18  trans_num              1296675 non-null  object \n",
      " 19  unix_time              1296675 non-null  int64  \n",
      " 20  merch_lat              1296675 non-null  float64\n",
      " 21  merch_long             1296675 non-null  float64\n",
      " 22  is_fraud               1296675 non-null  int64  \n",
      "dtypes: float64(5), int64(6), object(12)\n",
      "memory usage: 227.5+ MB\n",
      "None\n",
      "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
      "0           0   2019-01-01 00:00:18  2703186189652095   \n",
      "1           1   2019-01-01 00:00:44      630423337322   \n",
      "2           2   2019-01-01 00:00:51    38859492057661   \n",
      "3           3   2019-01-01 00:01:16  3534093764340240   \n",
      "4           4   2019-01-01 00:03:06   375534208663984   \n",
      "\n",
      "                             merchant       category     amt      first  \\\n",
      "0          fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer   \n",
      "1     fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie   \n",
      "2                fraud_Lind-Buckridge  entertainment  220.11     Edward   \n",
      "3  fraud_Kutch, Hermiston and Farrell  gas_transport   45.00     Jeremy   \n",
      "4                 fraud_Keeling-Crist       misc_pos   41.96      Tyler   \n",
      "\n",
      "      last gender                        street  ...      lat      long  \\\n",
      "0    Banks      F                561 Perry Cove  ...  36.0788  -81.1781   \n",
      "1     Gill      F  43039 Riley Greens Suite 393  ...  48.8878 -118.2105   \n",
      "2  Sanchez      M      594 White Dale Suite 530  ...  42.1808 -112.2620   \n",
      "3    White      M   9443 Cynthia Court Apt. 038  ...  46.2306 -112.1138   \n",
      "4   Garcia      M              408 Bradley Rest  ...  38.4207  -79.4629   \n",
      "\n",
      "   city_pop                                job         dob  \\\n",
      "0      3495          Psychologist, counselling  1988-03-09   \n",
      "1       149  Special educational needs teacher  1978-06-21   \n",
      "2      4154        Nature conservation officer  1962-01-19   \n",
      "3      1939                    Patent attorney  1967-01-12   \n",
      "4        99     Dance movement psychotherapist  1986-03-28   \n",
      "\n",
      "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
      "0  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
      "1  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
      "2  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
      "3  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071   \n",
      "4  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459   \n",
      "\n",
      "   is_fraud  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "is_fraud\n",
      "0    1289169\n",
      "1       7506\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "print(data.info())\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Check for class imbalance\n",
    "print(data['is_fraud'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a30b27f2-44f0-41d0-8916-a25d2c400202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the 'amt' and 'trans_date_trans_time' (or 'unix_time') features\n",
    "data['amt'] = StandardScaler().fit_transform(data['amt'].values.reshape(-1, 1))\n",
    "data['unix_time'] = StandardScaler().fit_transform(data['unix_time'].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f03124d7-2340-4a25-b8f5-993998fade3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0               0\n",
      "trans_date_trans_time    0\n",
      "cc_num                   0\n",
      "merchant                 0\n",
      "category                 0\n",
      "amt                      0\n",
      "first                    0\n",
      "last                     0\n",
      "gender                   0\n",
      "street                   0\n",
      "city                     0\n",
      "state                    0\n",
      "zip                      0\n",
      "lat                      0\n",
      "long                     0\n",
      "city_pop                 0\n",
      "job                      0\n",
      "dob                      0\n",
      "trans_num                0\n",
      "unix_time                0\n",
      "merch_lat                0\n",
      "merch_long               0\n",
      "is_fraud                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Define the feature matrix and target variable\n",
    "X = data.drop(columns='is_fraud')\n",
    "y = data['is_fraud']\n",
    "\n",
    "# Preprocessing: handle categorical and numerical data\n",
    "numeric_features = selector(dtype_include=['int64', 'float64'])\n",
    "categorical_features = selector(dtype_include=object)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Apply the transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply preprocessing pipeline to the data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4403451-0109-4498-a5ef-278f70aa1075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import joblib\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('fraudTrain.csv')\n",
    "test_data = pd.read_csv('fraudTest.csv')\n",
    "\n",
    "# Combine both datasets for preprocessing\n",
    "data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Define the feature matrix and target variable\n",
    "X = data.drop(columns='is_fraud')\n",
    "y = data['is_fraud']\n",
    "\n",
    "# Preprocessing: handle categorical and numerical data\n",
    "numeric_features = selector(dtype_include=['int64', 'float64'])\n",
    "categorical_features = selector(dtype_include=object)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Apply the transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply preprocessing pipeline to the data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Feature selection: Keep only the top k features\n",
    "k = 20  # You can adjust k to keep more or fewer features\n",
    "feature_selector = SelectKBest(score_func=chi2, k=k)\n",
    "X_selected = feature_selector.fit_transform(X_processed, y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the models with optimized parameters\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "random_forest = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)  # Parallel processing\n",
    "logistic_regression = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Use cross-validation to evaluate models and choose the best one\n",
    "models = {\n",
    "    \"Decision Tree\": decision_tree,\n",
    "    \"Random Forest\": random_forest,\n",
    "    \"Logistic Regression\": logistic_regression\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_auc = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Use cross-validation for a more robust evaluation\n",
    "    auc_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    mean_auc = auc_scores.mean()\n",
    "    \n",
    "    print(f\"{name} AUC (cross-validated): {mean_auc}\\n\")\n",
    "    \n",
    "    if mean_auc > best_auc:\n",
    "        best_auc = mean_auc\n",
    "        best_model = model\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_pred)}\\n\")\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'fraud_detection_model.pkl')\n",
    "\n",
    "print(f\"Best model saved with AUC: {best_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717a8e08-1205-401a-89a2-b9508c36da84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0               0\n",
      "trans_date_trans_time    0\n",
      "cc_num                   0\n",
      "merchant                 0\n",
      "category                 0\n",
      "amt                      0\n",
      "first                    0\n",
      "last                     0\n",
      "gender                   0\n",
      "street                   0\n",
      "city                     0\n",
      "state                    0\n",
      "zip                      0\n",
      "lat                      0\n",
      "long                     0\n",
      "city_pop                 0\n",
      "job                      0\n",
      "dob                      0\n",
      "trans_num                0\n",
      "unix_time                0\n",
      "merch_lat                0\n",
      "merch_long               0\n",
      "is_fraud                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import joblib\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('fraudTrain.csv')\n",
    "test_data = pd.read_csv('fraudTest.csv')\n",
    "\n",
    "# Combine both datasets for preprocessing\n",
    "data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Define the feature matrix and target variable\n",
    "X = data.drop(columns='is_fraud')\n",
    "y = data['is_fraud']\n",
    "\n",
    "# Preprocessing: handle categorical and numerical data\n",
    "numeric_features = selector(dtype_include=['int64', 'float64'])\n",
    "categorical_features = selector(dtype_include=object)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Apply the transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply preprocessing pipeline to the data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Decision Tree model\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Train the Random Forest model\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "logistic_regression = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models\n",
    "models = {\n",
    "    \"Decision Tree\": decision_tree,\n",
    "    \"Random Forest\": random_forest,\n",
    "    \"Logistic Regression\": logistic_regression\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_auc = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"AUC: {auc}\\n\")\n",
    "    \n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_model = model\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'fraud_detection_model.pkl')\n",
    "\n",
    "print(f\"Best model saved with AUC: {best_auc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
