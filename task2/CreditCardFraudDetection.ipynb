{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8c18fd-ce50-4bfe-8b2b-4ca809ff23db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lekhanaal\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "403a0e47-f6cd-4449-9788-d03535396237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('fraudTest.csv')\n",
    "data = pd.read_csv('fraudTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2ac118-d259-4e80-bd2c-e5b947189137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1296675 entries, 0 to 1296674\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   Unnamed: 0             1296675 non-null  int64  \n",
      " 1   trans_date_trans_time  1296675 non-null  object \n",
      " 2   cc_num                 1296675 non-null  int64  \n",
      " 3   merchant               1296675 non-null  object \n",
      " 4   category               1296675 non-null  object \n",
      " 5   amt                    1296675 non-null  float64\n",
      " 6   first                  1296675 non-null  object \n",
      " 7   last                   1296675 non-null  object \n",
      " 8   gender                 1296675 non-null  object \n",
      " 9   street                 1296675 non-null  object \n",
      " 10  city                   1296675 non-null  object \n",
      " 11  state                  1296675 non-null  object \n",
      " 12  zip                    1296675 non-null  int64  \n",
      " 13  lat                    1296675 non-null  float64\n",
      " 14  long                   1296675 non-null  float64\n",
      " 15  city_pop               1296675 non-null  int64  \n",
      " 16  job                    1296675 non-null  object \n",
      " 17  dob                    1296675 non-null  object \n",
      " 18  trans_num              1296675 non-null  object \n",
      " 19  unix_time              1296675 non-null  int64  \n",
      " 20  merch_lat              1296675 non-null  float64\n",
      " 21  merch_long             1296675 non-null  float64\n",
      " 22  is_fraud               1296675 non-null  int64  \n",
      "dtypes: float64(5), int64(6), object(12)\n",
      "memory usage: 227.5+ MB\n",
      "None\n",
      "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
      "0           0   2019-01-01 00:00:18  2703186189652095   \n",
      "1           1   2019-01-01 00:00:44      630423337322   \n",
      "2           2   2019-01-01 00:00:51    38859492057661   \n",
      "3           3   2019-01-01 00:01:16  3534093764340240   \n",
      "4           4   2019-01-01 00:03:06   375534208663984   \n",
      "\n",
      "                             merchant       category     amt      first  \\\n",
      "0          fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer   \n",
      "1     fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie   \n",
      "2                fraud_Lind-Buckridge  entertainment  220.11     Edward   \n",
      "3  fraud_Kutch, Hermiston and Farrell  gas_transport   45.00     Jeremy   \n",
      "4                 fraud_Keeling-Crist       misc_pos   41.96      Tyler   \n",
      "\n",
      "      last gender                        street  ...      lat      long  \\\n",
      "0    Banks      F                561 Perry Cove  ...  36.0788  -81.1781   \n",
      "1     Gill      F  43039 Riley Greens Suite 393  ...  48.8878 -118.2105   \n",
      "2  Sanchez      M      594 White Dale Suite 530  ...  42.1808 -112.2620   \n",
      "3    White      M   9443 Cynthia Court Apt. 038  ...  46.2306 -112.1138   \n",
      "4   Garcia      M              408 Bradley Rest  ...  38.4207  -79.4629   \n",
      "\n",
      "   city_pop                                job         dob  \\\n",
      "0      3495          Psychologist, counselling  1988-03-09   \n",
      "1       149  Special educational needs teacher  1978-06-21   \n",
      "2      4154        Nature conservation officer  1962-01-19   \n",
      "3      1939                    Patent attorney  1967-01-12   \n",
      "4        99     Dance movement psychotherapist  1986-03-28   \n",
      "\n",
      "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
      "0  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
      "1  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
      "2  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
      "3  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071   \n",
      "4  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459   \n",
      "\n",
      "   is_fraud  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "is_fraud\n",
      "0    1289169\n",
      "1       7506\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "print(data.info())\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Check for class imbalance\n",
    "print(data['is_fraud'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a30b27f2-44f0-41d0-8916-a25d2c400202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the 'amt' and 'trans_date_trans_time' (or 'unix_time') features\n",
    "data['amt'] = StandardScaler().fit_transform(data['amt'].values.reshape(-1, 1))\n",
    "data['unix_time'] = StandardScaler().fit_transform(data['unix_time'].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f03124d7-2340-4a25-b8f5-993998fade3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0               0\n",
      "trans_date_trans_time    0\n",
      "cc_num                   0\n",
      "merchant                 0\n",
      "category                 0\n",
      "amt                      0\n",
      "first                    0\n",
      "last                     0\n",
      "gender                   0\n",
      "street                   0\n",
      "city                     0\n",
      "state                    0\n",
      "zip                      0\n",
      "lat                      0\n",
      "long                     0\n",
      "city_pop                 0\n",
      "job                      0\n",
      "dob                      0\n",
      "trans_num                0\n",
      "unix_time                0\n",
      "merch_lat                0\n",
      "merch_long               0\n",
      "is_fraud                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Define the feature matrix and target variable\n",
    "X = data.drop(columns='is_fraud')\n",
    "y = data['is_fraud']\n",
    "\n",
    "# Preprocessing: handle categorical and numerical data\n",
    "numeric_features = selector(dtype_include=['int64', 'float64'])\n",
    "categorical_features = selector(dtype_include=object)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Apply the transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply preprocessing pipeline to the data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717a8e08-1205-401a-89a2-b9508c36da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import joblib\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('fraudTrain.csv')\n",
    "test_data = pd.read_csv('fraudTest.csv')\n",
    "\n",
    "# Combine both datasets for preprocessing\n",
    "data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Define the feature matrix and target variable\n",
    "X = data.drop(columns='is_fraud')\n",
    "y = data['is_fraud']\n",
    "\n",
    "# Preprocessing: handle categorical and numerical data\n",
    "numeric_features = selector(dtype_include=['int64', 'float64'])\n",
    "categorical_features = selector(dtype_include=object)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Apply the transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply preprocessing pipeline to the data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Decision Tree model\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Train the Random Forest model\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "logistic_regression = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models\n",
    "models = {\n",
    "    \"Decision Tree\": decision_tree,\n",
    "    \"Random Forest\": random_forest,\n",
    "    \"Logistic Regression\": logistic_regression\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_auc = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"AUC: {auc}\\n\")\n",
    "    \n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_model = model\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'fraud_detection_model.pkl')\n",
    "\n",
    "print(f\"Best model saved with AUC: {best_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b86d31c4-9067-46a4-921c-aeb014ad9df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0               0\n",
      "trans_date_trans_time    0\n",
      "cc_num                   0\n",
      "merchant                 0\n",
      "category                 0\n",
      "amt                      0\n",
      "first                    0\n",
      "last                     0\n",
      "gender                   0\n",
      "street                   0\n",
      "city                     0\n",
      "state                    0\n",
      "zip                      0\n",
      "lat                      0\n",
      "long                     0\n",
      "city_pop                 0\n",
      "job                      0\n",
      "dob                      0\n",
      "trans_num                0\n",
      "unix_time                0\n",
      "merch_lat                0\n",
      "merch_long               0\n",
      "is_fraud                 0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# You can adjust k to keep more or fewer features\u001b[39;00m\n\u001b[0;32m     52\u001b[0m feature_selector \u001b[38;5;241m=\u001b[39m SelectKBest(score_func\u001b[38;5;241m=\u001b[39mchi2, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m---> 53\u001b[0m X_selected \u001b[38;5;241m=\u001b[39m feature_selector\u001b[38;5;241m.\u001b[39mfit_transform(X_processed, y)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[0;32m     56\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_selected, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:567\u001b[0m, in \u001b[0;36m_BaseFilter.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    562\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    563\u001b[0m         X, y, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m], multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    564\u001b[0m     )\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(X, y)\n\u001b[1;32m--> 567\u001b[0m score_func_ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_func(X, y)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(score_func_ret, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpvalues_ \u001b[38;5;241m=\u001b[39m score_func_ret\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:266\u001b[0m, in \u001b[0;36mchi2\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m    264\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m(np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many((X\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;28;01melse\u001b[39;00m X) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput X must be non-negative.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Use a sparse representation for Y by default to reduce memory usage when\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# y has many unique classes.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m Y \u001b[38;5;241m=\u001b[39m LabelBinarizer(sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfit_transform(y)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X must be non-negative."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import joblib\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('fraudTrain.csv')\n",
    "test_data = pd.read_csv('fraudTest.csv')\n",
    "\n",
    "# Combine both datasets for preprocessing\n",
    "data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Define the feature matrix and target variable\n",
    "X = data.drop(columns='is_fraud')\n",
    "y = data['is_fraud']\n",
    "\n",
    "# Preprocessing: handle categorical and numerical data\n",
    "numeric_features = selector(dtype_include=['int64', 'float64'])\n",
    "categorical_features = selector(dtype_include=object)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Apply the transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply preprocessing pipeline to the data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Feature selection: Keep only the top k features\n",
    "k = 20  # You can adjust k to keep more or fewer features\n",
    "feature_selector = SelectKBest(score_func=chi2, k=k)\n",
    "X_selected = feature_selector.fit_transform(X_processed, y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the models with optimized parameters\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "random_forest = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)  # Parallel processing\n",
    "logistic_regression = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Use cross-validation to evaluate models and choose the best one\n",
    "models = {\n",
    "    \"Decision Tree\": decision_tree,\n",
    "    \"Random Forest\": random_forest,\n",
    "    \"Logistic Regression\": logistic_regression\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_auc = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Use cross-validation for a more robust evaluation\n",
    "    auc_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    mean_auc = auc_scores.mean()\n",
    "    \n",
    "    print(f\"{name} AUC (cross-validated): {mean_auc}\\n\")\n",
    "    \n",
    "    if mean_auc > best_auc:\n",
    "        best_auc = mean_auc\n",
    "        best_model = model\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_pred)}\\n\")\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'fraud_detection_model.pkl')\n",
    "\n",
    "print(f\"Best model saved with AUC: {best_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e160b5a-0e99-4d62-8fa7-5e12dd850d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0               0\n",
      "trans_date_trans_time    0\n",
      "cc_num                   0\n",
      "merchant                 0\n",
      "category                 0\n",
      "amt                      0\n",
      "first                    0\n",
      "last                     0\n",
      "gender                   0\n",
      "street                   0\n",
      "city                     0\n",
      "state                    0\n",
      "zip                      0\n",
      "lat                      0\n",
      "long                     0\n",
      "city_pop                 0\n",
      "job                      0\n",
      "dob                      0\n",
      "trans_num                0\n",
      "unix_time                0\n",
      "merch_lat                0\n",
      "merch_long               0\n",
      "is_fraud                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import joblib\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('fraudTrain.csv')\n",
    "test_data = pd.read_csv('fraudTest.csv')\n",
    "\n",
    "# Combine both datasets for preprocessing\n",
    "data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Define the feature matrix and target variable\n",
    "X = data.drop(columns='is_fraud')\n",
    "y = data['is_fraud']\n",
    "\n",
    "# Preprocessing: handle categorical and numerical data\n",
    "numeric_features = selector(dtype_include=['int64', 'float64'])\n",
    "categorical_features = selector(dtype_include=object)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Apply the transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply preprocessing pipeline to the data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Feature selection using RandomForestClassifier\n",
    "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "X_selected = feature_selector.fit_transform(X_processed, y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the models with optimized parameters\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "random_forest = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)  # Parallel processing\n",
    "logistic_regression = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Use cross-validation to evaluate models and choose the best one\n",
    "models = {\n",
    "    \"Decision Tree\": decision_tree,\n",
    "    \"Random Forest\": random_forest,\n",
    "    \"Logistic Regression\": logistic_regression\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_auc = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Use cross-validation for a more robust evaluation\n",
    "    auc_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    mean_auc = auc_scores.mean()\n",
    "    \n",
    "    print(f\"{name} AUC (cross-validated): {mean_auc}\\n\")\n",
    "    \n",
    "    if mean_auc > best_auc:\n",
    "        best_auc = mean_auc\n",
    "        best_model = model\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_pred)}\\n\")\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'fraud_detection_model.pkl')\n",
    "\n",
    "print(f\"Best model saved with AUC: {best_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a67ea-081e-4105-b3ae-e5ed4debb00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
