{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e520b18-2d79-4252-9617-d5e851272737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Free up memory\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "182b83c0-34e3-473e-b8bc-13d7205d9584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lekhanaal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lekhanaal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape after preprocessing: (54214, 4)\n",
      "Testing data shape after preprocessing: (54200, 3)\n",
      "Solution data shape after preprocessing: (54200, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Stopwords set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Text preprocessing function with re module\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove punctuation and numbers\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Tokenize (split by whitespace)\n",
    "        words = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return ''  # Return empty string if input is not a string\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "solution_df = pd.read_csv('solution.csv')\n",
    "\n",
    "# Fill missing values\n",
    "train_df.fillna('', inplace=True)\n",
    "test_df.fillna('', inplace=True)\n",
    "solution_df.fillna('', inplace=True)\n",
    "min_length = 25  \n",
    "keywords = ['drama','documentary','comedy','short','horror','thriller','action','western','reality-tv','family','adventure','music','romance','sci-fi','adult','crime','animation','sport','talk-show','fantasy','mystery','musical','biography','history','game-show',        \n",
    "'news','war' ]\n",
    "def filter_descriptions(df):\n",
    "    # Remove empty or null descriptions\n",
    "    df = df[df['DESCRIPTION'].notnull() & (df['DESCRIPTION'].str.strip() != '')]\n",
    "    # Filter based on length\n",
    "    df = df[df['DESCRIPTION'].str.len() >= min_length]\n",
    "    # Filter based on keywords\n",
    "    keyword_pattern = '|'.join(keywords)\n",
    "    df = df[df['DESCRIPTION'].str.contains(keyword_pattern, case=False, na=False)]\n",
    "    return df\n",
    "    # Print the shape of the dataframes to check the preprocessing\n",
    "print(f\"Training data shape after preprocessing: {train_df.shape}\")\n",
    "print(f\"Testing data shape after preprocessing: {test_df.shape}\")\n",
    "print(f\"Solution data shape after preprocessing: {solution_df.shape}\")\n",
    "# Preprocess the descriptions\n",
    "train_df['DESCRIPTION'] = train_df['DESCRIPTION'].astype(str).apply(preprocess_text)\n",
    "test_df['DESCRIPTION'] = test_df['DESCRIPTION'].astype(str).apply(preprocess_text)\n",
    "solution_df['DESCRIPTION'] = solution_df['DESCRIPTION'].astype(str).apply(preprocess_text)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))  # Reduced max_features\n",
    "X_train_text = tfidf_vectorizer.fit_transform(train_df['DESCRIPTION'])\n",
    "X_test_text = tfidf_vectorizer.transform(test_df['DESCRIPTION'])\n",
    "X_solution_text = tfidf_vectorizer.transform(solution_df['DESCRIPTION'])\n",
    "\n",
    "# Combine TF-IDF features with other features\n",
    "X_train = X_train_text\n",
    "X_test = X_test_text\n",
    "X_solution = X_solution_text\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df['GENRE'])\n",
    "y_test = le.transform(solution_df['GENRE'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd91902f-5eea-46eb-9141-0e57b8d8af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Convert the feature matrix to a sparse format if it isn't already\n",
    "X_train_sparse = csr_matrix(X_train_text)\n",
    "\n",
    "# Apply Truncated SVD for dimensionality reduction\n",
    "svd = TruncatedSVD(n_components=500, random_state=42)  # Adjust n_components as needed\n",
    "X_train_reduced = svd.fit_transform(X_train_sparse)\n",
    "# Apply SME on the reduced data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_reduced, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38971d6-cd58-4163-b7ee-ff6e2e897e03",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 701. MiB for an array with shape (367551, 250) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m ipca \u001b[38;5;241m=\u001b[39m IncrementalPCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Fit on the training data and then transform it\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X_train_res_reduced \u001b[38;5;241m=\u001b[39m ipca\u001b[38;5;241m.\u001b[39mfit_transform(X_train_res)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Now, transform the test and solution data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m X_test_reduced \u001b[38;5;241m=\u001b[39m ipca\u001b[38;5;241m.\u001b[39mfit_transform(X_test_text)           \u001b[38;5;66;03m# No need for .toarray()\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_incremental_pca.py:409\u001b[0m, in \u001b[0;36mIncrementalPCA.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mvstack(output)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_base.py:153\u001b[0m, in \u001b[0;36m_BasePCA.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m         X \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_\n\u001b[1;32m--> 153\u001b[0m X_transformed \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[0;32m    155\u001b[0m     X_transformed \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplained_variance_)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 701. MiB for an array with shape (367551, 250) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "ipca = IncrementalPCA(n_components=250, batch_size=1000)\n",
    "\n",
    "# Fit on the training data and then transform it\n",
    "X_train_res_reduced = ipca.fit_transform(X_train_res)\n",
    "\n",
    "# Now, transform the test and solution data\n",
    "X_test_reduced = ipca.fit_transform(X_test_text)           # No need for .toarray()\n",
    "X_solution_reduced = ipca.fit_transform(X_solution_text)   # No need for .toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6caf71f-c467-4abf-b19b-56774e1021f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_true):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b29e5-5aaf-494a-80b9-69fcabe11fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train_res = np.clip(X_train_res, a_min=0, a_max=None)\n",
    "X_test_reduced = np.clip(X_test_reduced, a_min=0, a_max=None)\n",
    "X_solution_reduced = np.clip(X_solution_reduced, a_min=0, a_max=None)\n",
    "# Train and evaluate Naive Bayes classifier\n",
    "param_grid_nb = {'alpha': [0.1, 0.5, 1.0]}\n",
    "grid_search_nb = GridSearchCV(MultinomialNB(), param_grid_nb, cv=5, scoring='accuracy')\n",
    "grid_search_nb.fit(X_train_res, y_train_res)\n",
    "nb_model = grid_search_nb.best_estimator_\n",
    "joblib.dump(nb_model, 'naive_bayes_model.pkl')\n",
    "nb_accuracy, nb_precision, nb_recall, nb_f1 = evaluate_model(nb_model, X_test_reduced, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407cb312-03a1-4857-82c2-9f0ab1958941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for negative values in the training data\n",
    "print(np.min(X_train_reduced))  # Should be >= 0 for MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748a8c5-b8d2-4562-a3c6-cb1d186e6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import joblib\n",
    "\n",
    "# Train and evaluate GaussianNB classifier\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_res, y_train_res)\n",
    "joblib.dump(nb_model, 'gaussian_nb_model.pkl')\n",
    "\n",
    "# Evaluate the model\n",
    "nb_accuracy, nb_precision, nb_recall, nb_f1 = evaluate_model(nb_model, X_test_reduced, y_test)\n",
    "print(f\"Accuracy: {nb_accuracy}\")\n",
    "print(f\"Precision: {nb_precision}\")\n",
    "print(f\"Recall: {nb_recall}\")\n",
    "print(f\"F1 Score: {nb_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874bc019-ae82-40dc-afa5-f1c04947f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Naive Bayes classifier\n",
    "param_grid_nb = {'alpha': [0.1, 0.5, 1.0]}\n",
    "grid_search_nb = GridSearchCV(GaussianNB(), param_grid_nb, cv=5, scoring='accuracy')\n",
    "grid_search_nb.fit(X_train_res, y_train_res)\n",
    "nb_model = grid_search_nb.best_estimator_\n",
    "joblib.dump(nb_model, 'naive_bayes_model.pkl')\n",
    "nb_accuracy, nb_precision, nb_recall, nb_f1 = evaluate_model(nb_model, X_test_reduced, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56412df-9e7f-42c0-9b48-a4231f54c663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c027a1-bc53-4b77-a6d1-4426bd53743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Logistic Regression classifier\n",
    "param_grid_lr = {'C': [0.1, 1, 10], 'solver': ['liblinear', 'saga']}\n",
    "grid_search_lr = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_lr, cv=5, scoring='accuracy')\n",
    "grid_search_lr.fit(X_train_res, y_train_res)\n",
    "lr_model = grid_search_lr.best_estimator_\n",
    "joblib.dump(lr_model, 'logistic_regression_model.pkl')\n",
    "lr_accuracy, lr_precision, lr_recall, lr_f1 = evaluate_model(lr_model, X_test_reduced, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840e406-cfc3-4c97-a549-57745790614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate SVM classifier\n",
    "param_grid_svm = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "grid_search_svm = GridSearchCV(SVC(), param_grid_svm, cv=5, scoring='accuracy')\n",
    "grid_search_svm.fit(X_train_res, y_train_res)\n",
    "svm_model = grid_search_svm.best_estimator_\n",
    "joblib.dump(svm_model, 'svm_model.pkl')\n",
    "svm_accuracy, svm_precision, svm_recall, svm_f1 = evaluate_model(svm_model, X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423c971-f61d-453f-b53e-d012d4fb064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF-IDF vectorizer and label encoder\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "joblib.dump(le, 'label_encoder.pkl')\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Naive Bayes - Accuracy: {nb_accuracy}, Precision: {nb_precision}, Recall: {nb_recall}, F1-score: {nb_f1}\")\n",
    "print(f\"Logistic Regression - Accuracy: {lr_accuracy}, Precision: {lr_precision}, Recall: {lr_recall}, F1-score: {lr_f1}\")\n",
    "print(f\"SVM - Accuracy: {svm_accuracy}, Precision: {svm_precision}, Recall: {svm_recall}, F1-score: {svm_f1}\")\n",
    "\n",
    "# Predict genre for a new plot summary\n",
    "def predict_genre(model, plot_summary):\n",
    "    plot_summary_preprocessed = preprocess_text(plot_summary)\n",
    "    plot_summary_tfidf = tfidf_vectorizer.transform([plot_summary_preprocessed])\n",
    "    plot_summary_reduced = pca.transform(plot_summary_tfidf.toarray())\n",
    "    predicted_genre = model.predict(plot_summary_reduced)\n",
    "    return le.inverse_transform(predicted_genre)[0]\n",
    "\n",
    "# Example usage\n",
    "new_plot_summary = \"A young boy discovers he has magical powers and attends a school for wizards.\"\n",
    "predicted_genre_nb = predict_genre(nb_model, new_plot_summary)\n",
    "predicted_genre_lr = predict_genre(lr_model, new_plot_summary)\n",
    "predicted_genre_svm = predict_genre(svm_model, new_plot_summary)\n",
    "\n",
    "print(f\"Predicted genre (Naive Bayes): {predicted_genre_nb}\")\n",
    "print(f\"Predicted genre (Logistic Regression): {predicted_genre_lr}\")\n",
    "print(f\"Predicted genre (SVM): {predicted_genre_svm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbcf3e8-f6fd-44b7-849b-8338e30535ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddf88a-47f4-46e3-aef0-67569340fbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
